<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom"><generator uri="http://jekyllrb.com" version="3.2.1">Jekyll</generator><link href="https://vincentherrmann.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://vincentherrmann.github.io/" rel="alternate" type="text/html" /><updated>2016-09-26T23:25:50+02:00</updated><id>https://vincentherrmann.github.io/</id><title>Vincent Herrmann</title><subtitle>Classically trained pianist and composer exploring the world of artificial intelligence</subtitle><author><name>Vincent Herrmann</name></author><entry><title>Werden Roboter jemals so werden wie wir? (German)</title><link href="https://vincentherrmann.github.io/robotics-essay/" rel="alternate" type="text/html" title="Werden Roboter jemals so werden wie wir? (German)" /><published>2016-09-26T00:00:00+02:00</published><updated>2016-09-26T00:00:00+02:00</updated><id>https://vincentherrmann.github.io/robotics-essay</id><content type="html" xml:base="https://vincentherrmann.github.io/robotics-essay/">&lt;p&gt;Dies ist ein kurzes Essay, das ich im Frühjahr zu einer vorgegebenen Fragestellung geschrieben habe:&lt;/p&gt;

&lt;h3 id=&quot;werden-roboter-jemals-so-werden-wie-wir-und-wollen-wir-das-uberhaupt&quot;&gt;Werden Roboter jemals so werden wie wir? Und wollen wir das überhaupt?&lt;/h3&gt;

&lt;p&gt;Die erste Frage sollte man nur äußerst vorsichtig beantworten. Wissenschaftliche und technologische Entwicklungen lassen sich kaum voraussagen. Durchbrüche und Paradigmenwechsel noch viel weniger. Ob es sich überhaupt lohnt, über diese Frage nachzudenken, scheint umstritten zu sein. Vor ein paar Tagen veröffentlichte der renommierte amerikanische Psychologe Robert Epstein einen Artikel, der Wellen bis in die Feuilletons deutscher Tageszeitungen geschlagen hat. Seine simple Grundaussage ist: “Das Gehirn ist kein Computer”. Darin erklärt er im Prinzip alle Spekulationen über die Annäherung von Robotern an den Menschen für unsinnig.&lt;/p&gt;

&lt;p&gt;Natürlich können wir nur im heutigen Erkenntnisstand geerdete Überlegungen anstellen, und niemand garantiert, dass diese nicht morgen komplett über den Haufen geworfen werden. Deshalb muss man aber noch lange nicht die Vorstellung der Berechenbarkeit kognitiver Vorgänge im menschlichen Gehirn für komplett unhaltbar erklären. Im Gegenteil: Nach meinem Kenntnisstand gibt es in den Neurowissenschaften keine Ergebnisse, die eindeutig im Widerspruch mit unseren physikalische, chemischen und biologischen Modellen stehen, und sich somit der Simulation durch einen Computer bzw. einen Roboter widersetzten. Occams vielzitiertes Rasiermesser gibt uns also doch eine gewisse Rechtfertigung dafür, schon heute nachzudenken, wie ähnlich sich Mensch und Maschine letztendlich werden können.
Die Funktionsweise von Mensch und Maschine ist natürlich grundverschieden. Das fängt schon damit an, dass man beim Computer zwischen Hard- und Software unterscheiden kann. Bei manchen Methoden des maschinellen Lernens fängt allerdings diese Grenze an zu verwischen: Es gibt eine eindeutige Parallele zwischen den Parametern künstlicher neuronaler Netz, in denen das gelernte Programm, die Software, codiert ist, und der Empfindlichkeit tatsächlicher Neuronen im Gehirn. Dabei handelt es sich um eine Vereinfachung, die intuitiv Sinn machen mag, bei der aber noch lange nicht klar ist, ob ihr nicht ganz entscheidende Aspekte abgehen. Wie groß die Gemeinsamkeit in der Art und Weise, wie diese beiden Systeme lernen, ist, lässt sich auch noch nicht abschätzen. Für den “Backpropagation”-Algorithmus beispielsweise, der effizientes Lernen in hierarchischen Berechnungsstrukturen erlaubt, ist kein entsprechender Mechanismus im menschlichen Gehirn bekannt, obwohl nicht ausgeschlossen ist, dass er auch dort verwendet wird (vgl. Hinton, “How to do Backpropagation in the Brain”, 2007).&lt;/p&gt;

&lt;p&gt;Schon heute sind die Einschränkungen des Menschen ein wichtiger Teil der Merkmale, die ihn von Maschinen unterscheiden. Uns bleibt nichts anderes übrig, zumindest solange wir keine Cyborgs sind, als das beste aus unserem zerbrechlichen Körper und fest eingeschlossenen Denkapparat zu machen. Wollen wir uns untereinander vernetzten, steht uns im Wesentlichen nur die Sprache als Schnittstelle mit verschwindend geringer Bandbreite zur Verfügung. Computer dagegen können bis zu einem gewissen Grad frei skaliert und miteinander verbunden werden. Doch in diesem Unterschied stecken vielleicht die Regulationen, welche am meisten zur spezifisch menschlichen Intelligenz, ihrer Subtilität, ihrem Abstraktionsvermögen, ihrer Resilienz, beitragen.&lt;/p&gt;

&lt;p&gt;Als Letztes bleibt sicherlich die Frage nach dem Bewusstsein. Ich persönlich bin nicht in der Lage mir vorzustellen, dass dieses Phänomen überhaupt erklärt werden könnte. Andererseits ist es durchausmöglich möglich, dass dies gar nicht notwendig ist um einen “selbst-bewussten” Roboter zu kreieren. Schmidhuber (Journal of Consciousness, 2012) argumentiert, es sei für komplexe lernende Agenten naheliegend, sich selbst in Bezug zur Umwelt zu stellen, und dafür eine Art Selbstsymbol zu schaffen. Wir wissen freilich nicht, ob das dann schon ein Bewusstsein wie das Unsere wäre.&lt;/p&gt;

&lt;p&gt;Eine künstliche Intelligenz, die in manchen Bereichen mit der menschlichen vergleichbar ist halte ich durchaus für möglich. So etwas wie eine asymptotische Annäherung von Maschinen an den Menschen kann ich mir dagegen nicht vorstellen. Wenn man das aber doch erreichen wollte, wären humanoide Roboter sicherlich am ehesten dazu in der Lage. Humanoid müsste dabei nicht nur die Erscheinung sein, sondern mehr noch die kognitive und sensomotorische Arbeitsweise. Ist das dem Menschen ähnlich Werden allerdings ein explizites Ziel, unterscheidet sich der Roboter gerade dadurch schon vom Menschen.&lt;/p&gt;

&lt;p&gt;Wäre das überhaupt ein sinnvolles Ziel? So lautet die zweite Frage. Was versprechen wir uns von Robotern, die genau so sind wie wir?
Sie würden unsere Werte haben, oder zumindest ließen diese sich antrainieren. Sie könnten die selbe Arbeit verrichten wie wir. Wir würden sie verstehen, sie verstünden uns. Es wäre ein intensiver, gleichwertiger Austausch zwischen Menschheit und Robotertum möglich. Der Gedanke ist verlockend: Wozu ein Mensch aufgrund seiner Konstitution nicht in der Lage ist, das könnten Roboter, die entsprechend konstruiert sind, übernehmen. Sie könnten den Meeresgrund oder fremde Planeten erforschen, womöglich zu anderen Sonnensystemen reisen oder in den Mikrokosmos eintauchen. Und, da sie uns ja gleich sind, wäre es so, als hätten Menschen dasselbe getan.&lt;/p&gt;

&lt;p&gt;Abgesehen davon, dass ich diese Zukunftsvision für unrealistisch halte, ist sie auch in vielerlei Hinsicht problematisch: Woher nehmen wir das Recht, Robotern die Arbeit zuzumuten, die wir selbst nicht verrichten wollen? Natürlich könnten Roboter, auch menschenähnliche, für bestimmte Tätigkeiten besser geeignet sein. Doch das bringt wiederum eine inhärente Ungleichheit mit sich, sowohl zwischen uns den Robotern, als auch zwischen den Robotern untereinander. Solche Ungleichheit birgt enormes ethisch-moralisches Konfliktpotential. Sind Roboter, die weniger können, auch weniger wert? Was passiert mit Robotern, die nicht gebraucht werden? Ein natürliches Ableben wird es nicht geben, und ist auch nur ein vorübergehendes Abschalten überhaupt zu verantworten? Würden nicht irgendwann Roboter konstruiert werden, die uns in allen Bereichen überlegen sind? Hat dann die Menschheit noch eine Daseinsberechtigung? Eine Überlebenschance?&lt;/p&gt;

&lt;p&gt;Wie ich schon erläutert habe, halte ich das für konstruierte Szenarien von nicht allzu großer Relevanz, erst recht nicht in absehbarer Zukunft. Seit Jahrtausenden benutzt die Menschheit Maschinen, die ihr in mancher Hinsicht besser funktionieren als wir. Sie funktionieren aber nicht nur besser, sondern auch anders, und gerade deshalb besser. Maschinen ergänzen also die menschlichen Fähigkeiten, sie ersetzen sie nicht. Man könnte sogar sagen, dass Tätigkeiten, die von Maschinen übernommen werden, eigentlich für Menschen nie passend waren. Hierbei bleibt natürlich die Frage, was letztlich als spezifisch Menschliches übrig bleiben wird. Aber dass uns eine künstliche Intelligenz ohne weiteres überrunden wird, wie unter dem Schlagwort der “Singularität” prophezeit wird, halte ich für unwahrscheinlich. Intelligenz ist etwas extrem Diffiziles, das nicht einfach wie mit einem Schieberegler hochgefahren werden kann. Pure Rechenleistung neigt schnell dazu in Leerlauf zu geraten. Jede Art von Intelligenz muss in engster Verknüpfung mit der Wirklichkeit bleiben. Deshalb kann der technologische Fortschritt, so zumindest meine einigermaßen optimistische Prognose, kann nur Hand in Hand mit der Menschheit geschehen.&lt;/p&gt;</content><author><name>Vincent Herrmann</name></author><summary>Dies ist ein kurzes Essay, das ich im Frühjahr zu einer vorgegebenen Fragestellung geschrieben habe:</summary></entry><entry><title>Wavelets I - From Filter Banks to the Dilation Equation</title><link href="https://vincentherrmann.github.io/wavelets-i/" rel="alternate" type="text/html" title="Wavelets I - From Filter Banks to the Dilation Equation" /><published>2016-09-21T00:00:00+02:00</published><updated>2016-09-21T00:00:00+02:00</updated><id>https://vincentherrmann.github.io/wavelets-i</id><content type="html" xml:base="https://vincentherrmann.github.io/wavelets-i/">&lt;p&gt;This is the first in what I hope will be a series of posts about wavelets, especially about various aspects of the Fast Wavelet Transform (FWT). Of course there are already plenty of resources, but I found them tending to be either simple implementation guides that do not touch on the many interesting and sometimes crucial connections. Or they are highly mathematical and definition-heavy, for a non-mathematician (such as me) it might be difficult to distill the important ideas.&lt;/p&gt;

&lt;p&gt;Here I will start with the implementation of the FWT, which is easily done with a cascading filter bank. From there I will try to develop the corresponding wavelet properties and get to the maybe most important aspect of the Multi-Resolution Analysis (MRA), namely the dilation and wavelet equations. I think this is a reasonable and intuitive approach. My assumption is that you know the very basic principles of the continuous wavelet transform, digital filters and the Z-transform. I just want to mention that this is my first posts of this kind in English and I would appreciate your comments if you find mistakes in this text or in the calculation, if something is unclear or if you have any kind of feedback or question!&lt;/p&gt;

&lt;h2 id=&quot;filter-banks&quot;&gt;Filter Banks&lt;/h2&gt;

&lt;p&gt;A finite impulse response (FIR) filter transforms a discrete signal $x(n)$ by convolving it with a finite series of values $h(k)$:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-1&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;y(n) = (h \ast x)(n) = \sum\limits_k h(k) x(n-k) \tag{1.1}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The basic building block of our filter bank is shown in this graphic:
&lt;!-- ![image](/images/FilterBankDB4.png) --&gt;&lt;/p&gt;

&lt;figure class=&quot; align-left&quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/wavelet-reconstruction-filter-bank.png&quot; alt=&quot;perfect reconstruction filter bank&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.1: A simple critically sampled filter bank with perfect reconstruction
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;The input signal $x(n)$ is split and fed into two different analysis filters $h_0$ and $h_1$. These two signals are downsampled by a factor of two (i.e. all values with odd $n$ are deleted). Now we have an intermediate representation of the input signal, consisting of two different signals $r_0$ and $r_1$, each with half the sample count. But if we do it right, since the total amount of samples remains the same, there could still be all information left from the input. To reconstruct the signal, the representations are now upsampled by a factor if two (a zero is inserted after each sample) and fed into the reconstruction filters $f_0$ and $f_1$. Then the outputs of these filters are added together.&lt;/p&gt;

&lt;p&gt;If possible, we want the output of this whole setup to be exactly the same as the input, except maybe a constant gain and delay, making it a perfect reconstruction (PR) filter bank. The simplest set of filters meeting this condition are the Haar filter: $h_0 = [1, 1]$, $h_1 = [-1, 1]$, $f_0 = [1, 1]$, $f_1 = [1, -1]$. You can check it yourself by feeding a single impulse signal ($x = […, 0, 0, 1, 0, 0, …]$) into the filter bank and seeing that the output is still an impulse. Because of the linearity of the system this means that signals, since they are just a linear combination of many impulses, will be perfectly reconstructed.&lt;/p&gt;

&lt;p&gt;Of course there are not than just the Haar filters that yield a PR filter bank. Let’s look at the Z-transform of Figure 1:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-2&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}
Y(z) &amp; = &amp; \frac{1}{2} \Big( H_0(z)F_0(z) + H_1(z)F_1(z) \Big) \: X(z) \: + \\ &amp; &amp;\frac{1}{2}\Big( H_0(-z)F_0(z) + H_1(-z)F_1(z) \Big) \: X(-z)
\end{array} \tag{1.2} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;More information is available &lt;a href=&quot;https://www.dsprelated.com/freebooks/sasp/Critically_Sampled_Perfect_Reconstruction.html#30743&quot;&gt;here&lt;/a&gt;. Our PR condition, with gain $g$ and a delay of $d$ samples, in terms of the Z-transform is&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-3&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;Y(z) =  g \: z^{-d} X(z) \tag{1.3}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;As we see, there is no use for the second term in $(1.2)$, i.e. the alias term proportional to $X(-z)$. The easiest way to get rid of it is to set&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-4&quot;&gt;&lt;/a&gt;
- $F_0(z) = -H_1(-z)$, which is equivalent to $f_0(n) = -(-1)^n h_1(n)$
&lt;span style=&quot;float:right;&quot;&gt;$(1.4)\;\;\;$&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-5&quot;&gt;&lt;/a&gt;
- $F_1(z) = H_0(-z)$, which is equivalent to $f_1(n) = (-1)^n h_0(n)$
&lt;span style=&quot;float:right;&quot;&gt;$(1.5)\;\;\;$&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;To have everything nice and compact we set $h_0$ and $h_1$ to be a so-called conjugate quadrature filter (CQF) pair. Here this means $h_1$ is $f_1$ in reverse ($L$ is the length of $h_0$ and must be even):&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-6&quot;&gt;&lt;/a&gt;
- $H_1(z) = z^{-(L-1)} F_1(z^{-1}) = -z^{-(L-1)} H_0(-z^{-1})$, or $h_1(n) = -(-1)^n h_0(L-1-n)$
&lt;span style=&quot;float:right;&quot;&gt;$(1.6)\;\;\;$&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Now $f_0$, $f_1$ and $h_1$ can be all expressed in terms of $h_0$. An additional benefit is that $h_0$ and $h_1$ are orthogonal:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\sum_n h_0(n)h_1(n) = \sum_n h_0(n)\Big(-(-1)^n h_0(L-1-n)\Big)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= -h_0(0)h_0(L-1) + h_0(1)h_0(L-2) - ...-h_0(L-2)h_0(1)+h_0(L-1)h_0(0) = 0&lt;/script&gt;

&lt;p&gt;Equation $(1.2)$  combined with the PR condition $(1.3)$ becomes:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Y(Z) = \frac{1}{2} \Big( H_0(z)F_0(z) + H_1(z)F_1(z) \Big) \: X(z)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \frac{1}{2} \Big( -H_0(z)z^{-(L-1)} H_0(z^{-1}) \; - \; z^{-(L-1)} H_0(-z^{-1})H_0(-z) \Big) \: X(z) = g \: z^{-d} X(z)&lt;/script&gt;

&lt;p&gt;If we set $d=L-1$ and $g=-1$, we can write&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-7&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;H_0(z)H_0(z^{-1}) \; + \; H_0(-z^{-1})H_0(-z) = 2 \tag{1.7}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This constraint is not quite as straightforward to translate for $h_0$. Using the definition of the Z-transform we can write&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{cl}
&amp; H_0(z)H_0(z^{-1}) \; + \; H_0(-z^{-1})H_0(-z) \\
= &amp; \left( \sum_{n} h_0(n) z^{-n} \right) \left( \sum_{n} h_0(n) z^{n} \right) + \left( \sum_{n}-  h_0(n) z^{-n} \right) \left( \sum_{n} - h_0(n) z^{n} \right) \\
= &amp; \sum_m \sum_n h_0(n)h_0(n+m) z^m + \sum_m \sum_n -h_0(n)h_0(n+m) z^m \\
= &amp; 2 \sum_{\mathrm{even} \:m} \sum_n h_0(n)h_0(n+m) z^m = 2
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Because it can’t depend on $z$, $\sum_n h_0(n)h_0(n+m)$ has to be zero for $m \neq 0$. With $k := \frac{1}{2}m$, our perfect reconstruction constraint for $h_0$ is:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-8&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;\sum_n h_0(n)h_0(n + 2k) = \delta (k) = \begin{cases} 1, \; k = 0 \\ 0, \; k\neq 0 \end{cases} \tag{1.8}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Which means $h_0$ has to be orthogonal to versions of itself shifted by an even number of samples. This orthogonality condition, in combination with $(1.4)$, $(1.5)$ and $(1.6)$ is sufficient for creating a set of filters for a PR filter bank. A procedure for getting the $h_0$ coefficients with additional beneficial features will be explained in my next blog post.&lt;/p&gt;

&lt;p&gt;As you can see, I use specific filters in the graphics. These are the filter coefficients of the Daubechies 4 wavelet, each one in a different color.&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;color:#37abc8&quot;&gt;$h_0(0) = 0.6830127$&lt;/span&gt; &lt;br /&gt;
&lt;span style=&quot;color:#d45500&quot;&gt;$h_0(1) = 1.1830127$&lt;/span&gt; &lt;br /&gt;
&lt;span style=&quot;color:#5aa02c&quot;&gt;$h_0(2) = 0.3169873$&lt;/span&gt; &lt;br /&gt;
&lt;span style=&quot;color:#ab37c8&quot;&gt;$h_0(3) = -0.1830127$&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;You can check that they meet the $(1.8)$ condition. I picked these because of the manageable filter length of four, and their characteristic shape. My aim is to anchor the abstractions with some concrete elements and thus making it easier and more intuitive to grasp.&lt;/p&gt;

&lt;p&gt;The representations $r_0$ and $r_1$ are themselves signals that can be fed into a similar filter bank module. We can do this an arbitrary number of times, resulting in more and more representations of coarser and coarser resolution.
&lt;!-- ![image](/images/FilterCascadeDB4Analysis.png) --&gt;&lt;/p&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/wavelet-cascaded-filter-bank-analysis.png&quot; alt=&quot;analysis part of a cascaded perfect reconstruction filter bank&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.2: Cascading analysis filter bank
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;Reconstruction works the same way. And if all filter bank modules reconstruct the signal perfectly, so does the whole cascade.
&lt;!-- ![image](/images/FilterCascadeDB4Synthesis.png) --&gt;&lt;/p&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/wavelet-cascaded-filter-bank-synthesis.png&quot; alt=&quot;synthesis part of a cascaded perfect reconstruction filter bank&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.3: Cascading reconstruction filter bank
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;h2 id=&quot;multi-resolution-convolution&quot;&gt;Multi Resolution Convolution&lt;/h2&gt;
&lt;p&gt;In this section we will concentrate on the $h_0$ filter. So let’s look at the upper half of Figure 2 (shaded in yellow). What has to happen to compute one sample, say the one with index 0, of the representation signal $r_{000}$? We can draw a graph describing the operation as a computational graph:&lt;/p&gt;

&lt;!-- ![image](/images/WaveletGraph1Int.png) --&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/wavelet-convolution-graph-int.png&quot; alt=&quot;computational graph of a multi-level convolution with down-sampling&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.4: Computational graph for calculating one ouput sample of the third filter bank stage
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;The white squares illustrate the signal at different representation states, which I have slightly renamed for convenience: $s_0 = x, s_1 = r_0, s_2 = r_{00}$ and in general $s_i$ being the representation $r$ with $i$ subscript zeros, meaning the output of the $i$th analysis filter $h_0$. A colored line stands for a multiplication with the corresponding filter coefficient, following the specified convolution and downsampling procedure&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-9&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;s_{n+1} = \sum\limits_k h_0(k) s_n(2j - k) \tag{1.9}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;It should be clear how the graph would look like if we added more layers and looked at the sample 0 of the innermost representation signal. The graph would grow upwards, getting denser and denser at the top, but the bottom part stayed the same, only the labels of the representations would change.&lt;/p&gt;

&lt;p&gt;Technically, our topic is wavelets. At their core idea, wavelet representations do not build on one another. A scaled and shifted version of a wavelet is used always on the original signal, the cascading approach is just an implementation trick. This means for our filter bank it is also useful to examine how the direct link from the signal to every representation level looks like. For that, we have to look at Figure 4 in a different way: All the operations for generating the bottom sample 0 from the input signal $x$ - since they are all linear - can be combined into a single long filter $c_n$, where $n$ is the number of considered representation stages. To get the coefficients of this combined filter, we look at all the paths connecting a square at the level $c_n$ to the bottom square. These paths all have $n$ colored segments. The coefficients corresponding to the color of the segments are multiplied and these products are added for each different path. Note that now the white squares don’t depict the signal in its various representations anymore, but the coefficients of the combined filter $c_n$. For example, if we take $c_3[-10]$, there are four paths to $c_0[0]$:&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;font-family:Arial&quot;&gt;
[-10]&lt;span style=&quot;color: #37abc8;&quot;&gt;——&lt;/span&gt;[-5]&lt;span style=&quot;color: #d45500;&quot;&gt;——&lt;/span&gt;[-2]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[0] &lt;br /&gt;
[-10]&lt;span style=&quot;color: #37abc8;&quot;&gt;——&lt;/span&gt;[-5]&lt;span style=&quot;color: #ab37c8;&quot;&gt;——&lt;/span&gt;[-1]&lt;span style=&quot;color: #d45500;&quot;&gt;——&lt;/span&gt;[0] &lt;br /&gt;
[-10]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[-4]&lt;span style=&quot;color: #37abc8;&quot;&gt;——&lt;/span&gt;[-2]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[0] &lt;br /&gt;
[-10]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[-4]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[-1]&lt;span style=&quot;color: #d45500;&quot;&gt;——&lt;/span&gt;[0] &lt;br /&gt;&lt;br /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;This results in a filter coefficient
&lt;script type=&quot;math/tex&quot;&gt;c_3[-10] = h_0(0) h_0(1) h_0(2) + h_0(0) h_0(3) h_0(1) + h_0(2) h_0(0) h_0(2) + h_0(2) h_0(2) h_0(1)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The resulting factor is exactly how an input sample is scaled while traveling through the graph from top to bottom.&lt;/p&gt;

&lt;p&gt;The efficient way to compute the coefficients for $c_n$ is working the way up the graph and using the following rule (all values of $c_n$ with non-integer indices are zero):&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-10&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;c_{n+1}(j) = \sum\limits_k h_0(k) c_n \left( \frac{j+k}{2} \right) \tag{1.10}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The length of the filter $c_n$ more than doubles for each subsequent level. To again get closer to the idea of wavelets, which are first defined at a certain size and then scaled, we do not want a ever growing discrete filter, but a function that ideally will be continuous at the limit. To achieve this, we have to introduce a re-parametrization of the filter: $x_n(t_n) = c_n(j)$, where $t_n = -2^{-n}j$, which also means $t_n = 2t_{n+1}$.&lt;/p&gt;

&lt;!-- ![image](/images/WaveletGraph1Float.png) --&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/wavelet-convolution-graph-float.png&quot; alt=&quot;computational graph with new continuous indexing&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.5: The same graph as figure 4, with new indexing
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;We have to adapt our construction progression for the filter functions $x_n$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}
c_{n+1}(j) &amp; = &amp; \sum\limits_k h_0(k) c_n \left( \frac{j+k}{2} \right) \\
x_{n+1}(t_{n+1}) &amp; = &amp; \sum\limits_k h_0(k) c_n \left( \frac{j+k}{2} \right) \\
&amp; = &amp; \sum\limits_k h_0(k) x_n \left( -2^{-n} \frac{j+k}{2} \right) \\
&amp; = &amp; \sum\limits_k h_0(k) x_n \left( \frac{-2^{-n}j}{2} + \frac{-2^{-n}k}{2} \right) \\
&amp; = &amp; \sum\limits_k h_0(k) x_n \left( \frac{t_n}{2} + \frac{-2^{-n}k}{2} \right) \\
&amp; = &amp; \sum\limits_k h_0(k) x_n \left( t_{n+1} + \frac{-2^{-n}k}{2} \right) \\
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Or when we set $t := t_{n+1}$&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-11&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;x_{n+1}(t) = \sum\limits_k h_0(k) x_n(t - 2^{-n-1}k) \tag{1.11}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;We have to give this sequence a starting value, we use $x_0(0) = 1$. Then $x_1$ has just the $h_0$ filter coefficients as values. The next graphic shows the first several levels of $x$ and the converged progression:&lt;/p&gt;

&lt;!-- ![image](/images/DB4XApproximation.png) --&gt;

&lt;figure class=&quot; align-left&quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/daubechies-4-scaling-approximation.png&quot; alt=&quot;several approximations of the Daubechies 4 scaling function&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.6: Several approximation levels of $x$, from the filter coefficients to the scaling function
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;These are, in the sense of a discrete continuous wavelet transform, the actual “wavelets” used, for each representation a different one, although they converge quite quickly.&lt;/p&gt;

&lt;h2 id=&quot;the-dilation-identity&quot;&gt;The Dilation Identity&lt;/h2&gt;

&lt;p&gt;The $2^{-n-1}$ factor in $(1.11)$ prevents this progression to be elegantly expressed as a limit for $n \rightarrow \infty$. So we will try to find a better suited equivalent progression. The first step, calculating $x_1$ from $x_0$, could also be achieved by $x_1(t) = \sum_k h_0(k) x_0(2t - k)$, as we can easily see. This currently seems somewhat arbitrary, but let’s generalize this relation nevertheless to a construction progression of different function $\phi$:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-12&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;\phi_{n+1}(t) =   \sum\limits_k h_0(k) \phi_n(2t - k) \tag{1.12}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Notice that it has the same form as $(1.9)$. I think this resemblance can be a bit misleading, as blurs the distinction between the filter bank implementation and the underlying mathematical framework of the MRA. Looking at the corresponding graph shows that the link between $x_n$ and $\phi_n$ is less obvious at it may seem at the first superficial glance:&lt;/p&gt;

&lt;!-- ![image](/images/WaveletGraph2Float.png) --&gt;

&lt;figure class=&quot; &quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/wavelet-dilation-graph-float.png&quot; alt=&quot;computational graph constructed with the dilation rule&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.7: Computational graph based on filter dilation
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;The connections are completely different from the previous graph, except for the first step. But the remarkable thing is that the paths connecting value of $\phi_n$ to $\phi_0$ have exactly the same color combinations as the $x_n$ graph. See for yourself! We can look again $\phi_3(1.250)$, which is equivalent to our previous example $c_3(-10)$. The paths connecting this node to $\phi_0(0)$ are:&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;font-family:Arial&quot;&gt;
[1.250]&lt;span style=&quot;color: #d45500;&quot;&gt;——&lt;/span&gt;[1.50]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[1.0]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[0] &lt;br /&gt;
[1.250]&lt;span style=&quot;color: #d45500;&quot;&gt;——&lt;/span&gt;[1.50]&lt;span style=&quot;color: #ab37c8;&quot;&gt;——&lt;/span&gt;[0.0]&lt;span style=&quot;color: #37abc8;&quot;&gt;——&lt;/span&gt;[0] &lt;br /&gt;
[1.250]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[0.50]&lt;span style=&quot;color: #37abc8;&quot;&gt;——&lt;/span&gt;[1.0]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[0] &lt;br /&gt;
[1.250]&lt;span style=&quot;color: #5aa02c;&quot;&gt;——&lt;/span&gt;[0.50]&lt;span style=&quot;color: #d45500;&quot;&gt;——&lt;/span&gt;[0.0]&lt;span style=&quot;color: #37abc8;&quot;&gt;——&lt;/span&gt;[0] &lt;br /&gt;
&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;And with that we have&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\phi_3(-1.250) = h_0(1) h_0(2) h_0(2) + h_0(1) h_0(3) h_0(0) + h_0(2) h_0(0) h_0(2) + h_0(2) h_0(1) h_0(0)&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;= c_3(-10)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The order of the paths is jumbled, as are the colors of the path segments (those are in fact exactly reversed). But in the end they generate the same result. But while we can verify the identity $\phi_n(t) = x_n(t)$ for $n={1, 2, 3}$, this seems a bit magical and is nothing more than a nice observation. We need a proof that the identity holds for all $n$.&lt;/p&gt;

&lt;p&gt;Let’s start with our two progressions $(1.11)$ and $(1.12)$:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}
x_{n+1}(t) &amp; = &amp; \sum\limits_k h_0(k) \: x_n(t-2^{-n-1}k) \\
\phi_{n+1}(t) &amp; = &amp; \sum\limits_k h_0(k) \: \phi_n(2t-k)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;We assume that $x_n = \phi_n$. They are now interchangeable and we swap them.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}
 x_{n+1}(t) &amp; = &amp; \sum\limits_k h_0(k) \: \phi_n(t-2^{-n-1}k) \\
\phi_{n+1}(t) &amp; = &amp; \sum\limits_k h_0(k) \: x_n(2t-k)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;Then we go back one more level and plug in the recursive step that defines $\phi_n$ or $x_n$ in terms of $\phi_{n-1}$ or $x_{n-1}$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}
 x_{n}(t) &amp; = &amp; \sum\limits_k h_0(k) \: x_{n-1}(t-2^{-n}k) \\
\phi_{n}(t) &amp; = &amp; \sum\limits_k h_0(k) \: \phi_{n-1}(2t-k)
\end{array} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rclcl}
 x_{n+1}(t) &amp; = &amp; \sum\limits_{k,l} h_0(k) h_0(l) \: \phi_{n-1}(2(t-2^{-n-1}k)-l) &amp; = &amp; \sum\limits_{k,l} h_0(k) h_0(l) \: \phi_{n-1}(2t-2^{-n}k-l) \\
\phi_{n+1}(t) &amp; = &amp; \sum\limits_{k,l} h_0(k) h_0(l) \: x_{n-1}((2t-l) - 2^{-n}k) &amp; = &amp; \sum\limits_{k,l} h_0(k) h_0(l) \: x_{n-1}(2t-2^{-n}k -l)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;As we see, the arguments of $\phi_{n-1}$ and $x_{n-1}$ on the right are the same. This means, if we additionally assume $\phi_{n-1} = x_{n-1}$, then $\phi_{n+1} = x_{n+1}$. In short:&lt;/p&gt;

&lt;p&gt;If $\phi_{n} = x_{n}$ and $\phi_{n-1} = x_{n-1}$, then $\phi_{n+1} = x_{n+1}$&lt;/p&gt;

&lt;p&gt;We have already seen that our assumptions hold for the first two levels, i.e. $\phi_0(t) = x_0(t)$ and $\phi_1(t) = x_1(t)$. With that, as we showed, we have $\phi_2(t) = x_2(t)$, and as a consequence of that $\phi_3(t) = x_3(t)$, and so on for all $n$. Therefore we have proved by induction that indeed:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-13&quot;&gt;&lt;/a&gt;
$\phi_n(t) = x_n(t); \; n \geqslant 0, t \in R$, if $\phi_0(0) = x_0(0) = 1$ and $\phi_0(t) = x_0(t) = 0;  \forall t \neq 0$
&lt;span style=&quot;float:right;&quot;&gt;$(1.13)\;\;\;$&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-dilation-and-wavelet-equation&quot;&gt;The Dilation and Wavelet Equation&lt;/h2&gt;

&lt;p&gt;For $n \to \infty$, $\phi_n$ converges to a continuous function with support $[0, K-1]$, where $K$ is the number of coefficients of $h_0$, and $x_n(0) = x_n(K-1) = 0$ (the support of a function is the interval where it is different from zero). Let’s call this function simply $\phi$. This converged progression still has to hold the construction criterion:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-14&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;\phi(t) = \sum\limits_k h_0(k) \phi(2t-k) \tag{1.14}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is the dilation equation, the workhorse in the theory of FWT and multi resolution analysis (MRA). And $\phi$ itself is the scaling function of the FWT, in our concrete examples the DB4 scaling function. The dilation equation says that the scaling function has a fractal self-similarity: it can be constructed of versions of itself, compressed by a factor of two, shifted and scaled by the $h_0$ coefficients.&lt;/p&gt;

&lt;!-- ![image](/images/DB4SynthesisFractals.png)
![image](/images/DB4SynthesisSum.png) --&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/daubechies-4-scaling-components.png&quot; alt=&quot;fractal components of the Daubechies 4 scaling function&quot; /&gt;
    
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/daubechies-4-scaling-sum.png&quot; alt=&quot;sum of the fractal components of the Daubechies 4 scaling function&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.8: Fractal construction of the DB4 scaling function from versions of a different scale, scaled by the $h_0$ coefficients
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;We of course have a way to approximate $\phi$ by iterating (1.12). This is called the cascading algorithm, and the iterations will, as we showed, look exactly like in Figure 6. While it is not possible to write the scaling function out explicitly, we can get can exact values by solving a system of linear equations derived from the dilation equation.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}
\phi(0) &amp; = &amp; h_0(0) \phi(0) \\
\phi(1) &amp; = &amp; h_0(2) \phi(0) + h_0(1) \phi(1) + h_0(0) \phi(2) \\
\phi(2) &amp; = &amp; h_0(3) \phi(1) + h_0(2) \phi(2) + h_0(1) \phi(3) \\
\phi(3) &amp; = &amp; h_0(3) \phi(3)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\small \begin{array}{rcrcrcrr}
(h_0(0) - 1) \;  \phi(0) &amp; + &amp; 0 \;  \phi(1) &amp; + &amp; 0 \;  \phi(2) &amp; + &amp; 0 \;  \phi(3) &amp; = 0
\\ h_0(2) \;  \phi(0) &amp; + &amp;  (h_0(1) - 1) \;  \phi(1) &amp; + &amp; h_0(0) \;  \phi(2) &amp; + &amp; 0 \;  \phi(3) &amp; = 0
\\ 0 \;  \phi(0) &amp; + &amp;  h_0(3) \;  \phi(1) &amp; + &amp; (h_0(2)-1) \;  \phi(2) &amp; + &amp; h_0(1) \;  \phi(3) &amp; = 0
\\ 0 \;  \phi(0) &amp; + &amp; 0 \;  \phi(1) &amp; + &amp; 0 \;  \phi(2) &amp; + &amp; (h_0(3)-1) \;  \phi(3) &amp; = 0
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;This system is homogeneous, meaning it has only zeros on the right side, and has no distinct solution. We have modify it slightly:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\small \begin{array}{rcrcrcrr}
(h_0(0) - 1) \; \phi(0) &amp; + &amp; 0 \; \phi(1) &amp; + &amp; 0 \; \phi(2) &amp; + &amp; (h_0(3)-1) \; \phi(3) &amp; = 0
\\ h_0(2) \; \phi(0) &amp; + &amp;  (h_0(1) - 1) \; \phi(1) &amp; + &amp; h_0(0) \; \phi(2) &amp; + &amp; 0 \; \phi(3) &amp; = 0
\\ 0 \; \phi(0) &amp; + &amp;  h_0(3) \; \phi(1) &amp; + &amp; (h_0(2)-1) \; \phi(2) &amp; + &amp; h_0(1) \; \phi(3) &amp; = 0
\\ 1 \; \phi(0) &amp; + &amp; 1 \; \phi(1) &amp; + &amp; 1 \; \phi(2) &amp; + &amp; 1 \; \phi(3) &amp; = 1
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;I have added the first and last equation together and added an additional constraint, namely that the sum of the four variables should be 1. This system can be solved and gives us the values of $\phi$ at the four integer positions. We plug these values in the dilation equation to get the values that lie exactly halfway between them and repeat this procedure. This way we have very quickly a large amount of exact values of $\phi$.&lt;/p&gt;

&lt;p&gt;We have not yet looked at the signal representations obtained with the $h_1$ filter, the $r_{…1}$ in Figure 2. We cannot define something like a scaling function for $h_1$, because the cascading algorithm does not converge for a highpass filter. But we can easily see what happens if we analyze a representation that itself was constructed with the $h_0$ scaling function $\phi$ (so basically one pass through the $h_1$ filter after infinitely many passes through $h_0$). For this we take the proof leading to $(1.13)$ and use it with adapted progressions:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{array}{rcl}
w_{n+1}(t) &amp; = &amp; \sum\limits_k h_1(k) \: x_n(t-2^{-n-1}k) \\
\psi_{n+1}(t) &amp; = &amp; \sum\limits_k h_1(k) \: \phi_n(2t-k)
\end{array} %]]&gt;&lt;/script&gt;

&lt;p&gt;The proof works just the same, and we can write:&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;eq-15&quot;&gt;&lt;/a&gt;
&lt;script type=&quot;math/tex&quot;&gt;\psi(t) = \sum\limits_k h_1(k) \phi(2t-k) \tag{1.15}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;This is the wavelet equation, defining the wavelet function $\psi$ of our FWT. According to the this equation, the wavelet function $\psi$ can also be constructed from compressed and scaled version version of $\phi$, this time scaled by $h_1$.&lt;/p&gt;

&lt;!-- ![image](/images/DB4WaveletSynthesisFractals.png)
![image](/images/DB4WaveletSynthesisSum.png) --&gt;

&lt;figure class=&quot;half &quot;&gt;
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/daubechies-4-wavelet-components.png&quot; alt=&quot;fractal components of the Daubechies 4 wavelet function&quot; /&gt;
    
  
    
      &lt;img src=&quot;https://vincentherrmann.github.io/images/daubechies-4-wavelet-sum.png&quot; alt=&quot;sum of the fractal components of the Daubechies 4 wavelet function&quot; /&gt;
    
  
  
    &lt;figcaption&gt;Fig.9: Construction of the DB4 wavelet function from compressed scaling functions, scaled by the $h_1$ coefficients
&lt;/figcaption&gt;
  
&lt;/figure&gt;

&lt;p&gt;The scaling function $\phi$ and the wavelet function $\psi$ are theoretical limits that never actually occur in a practical filter bank implementation of the FWT, not even as a discretized or subsampled version. This is an important distinction from a discrete implementation of the continuous wavelet transform, where the scaled wavelets are explicitly used.&lt;/p&gt;

&lt;p&gt;The fact that the actually (implicitly) used filter $\phi_n$ with finite $n$ vary from $\phi$, can be seen as a discretization artifact. A signal with a higher sample rate uses a better approximation of $\phi$ at a certain scale, because it must have gone through more precedent filters to reach this level. If we could use the filter banks on a infinitesimally sampled signal, all filters would have had an infinite count of filters before them, making the effective filter be equal to $\phi$.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;We have seen the conditions $(1.4)$ - $(1.8)$ for the filters of a cascading filter bank implementation of the FWT. Those are critical and sufficient to make it work. Then we have tried to look at this implementation from a wavelet point of view. For this we have introduced the identity $(1.13)$, which then led to the dilation equation $(1.14)$ and $(1.15)$. That gives us an analytic perspective for the converged state of our algorithm. I hope this was interesting for now, although it might not yet be actually useful. In the next blog post, however, I will explain how these results can help us to design better filters.&lt;/p&gt;

&lt;h2 id=&quot;useful-resources&quot;&gt;Useful Resources&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://people.math.ethz.ch/~blatter/Wavelets.pdf&quot;&gt;Blatter, Christian: Wavelets - Eine Einführung (link to pdf, german)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://orion.math.iastate.edu/keinert/book.html&quot;&gt;Keinert, Fritz: Wavelets and Multiwavelets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.math.kit.edu/ianm3/lehre/wavelets2008w/&quot;&gt;Rieder, Andreas: Wavelets - Theory and Applications&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.dsprelated.com/freebooks/sasp/&quot;&gt;Smith III, Julius O.: Spectral Audio Processing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www-math.mit.edu/~gs/papers/siamrev.pdf&quot;&gt;Strang, Gilbert: Wavelets and Dilation Equations: A Brief Introduction (link to pdf)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.maths-in-industry.org/miis/377/1/Orthogonal-wavelets-via-filter-banks.pdf&quot;&gt;Winkler, Joab A.: Orthogonal Wavelets via Filter Banks: Theory and Applications (link to pdf)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Vincent Herrmann</name></author><category term="wavelets" /><category term="dsp" /><summary>Derivation of the dilation and wavelet equation from an implementation of the Fast Wavelet Transform</summary></entry></feed>
